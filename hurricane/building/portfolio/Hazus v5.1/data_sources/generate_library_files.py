"""nerates Hazus Hurricane damage and loss database files."""

# This code was written before we began enforcing more strict linting
# standards. pylint warnings are ignored for this file.

# pylint: skip-file

from __future__ import annotations

import json
import shutil
import warnings
from copy import deepcopy

import numpy as np
import pandas as pd
from scipy.optimize import minimize
from scipy.stats import norm
from tqdm import tqdm

warnings.simplefilter('ignore', RuntimeWarning)
warnings.simplefilter('ignore', FutureWarning)


def parse_description(descr, parsed_data):  # noqa: C901
    """
    Parse the descr string and store params in row Series.

    Parameters
    ----------
    descr: string
        Hurricane archetype description from Hazus's raw database.
    parsed_data: pd.Series
        A pandas Series to collect parsed data.

    """
    # Roof Shape
    if 'rsflt' in descr:
        parsed_data['roof_shape'] = 'flt'
        descr = descr.replace('rsflt', '')
    elif 'rsgab' in descr:
        parsed_data['roof_shape'] = 'gab'
        descr = descr.replace('rsgab', '')
    elif 'rship' in descr:
        parsed_data['roof_shape'] = 'hip'
        descr = descr.replace('rship', '')

    # Secondary Water Resistance
    if 'swrys' in descr:
        parsed_data['sec_water_res'] = True
        descr = descr.replace('swrys', '')
    elif 'swrno' in descr:
        parsed_data['sec_water_res'] = False
        descr = descr.replace('swrno', '')

    # Roof Deck Attachment
    if 'rda6d' in descr:
        parsed_data['roof_deck_attch'] = '6d'
        descr = descr.replace('rda6d', '')
    elif 'rda8d' in descr:
        parsed_data['roof_deck_attch'] = '8d'
        descr = descr.replace('rda8d', '')
    elif 'rda6s' in descr:
        parsed_data['roof_deck_attch'] = '6s'
        descr = descr.replace('rda6s', '')
    elif 'rda8s' in descr:
        parsed_data['roof_deck_attch'] = '8s'
        descr = descr.replace('rda8s', '')

    # Roof Deck Attachment - Alternative Description
    if 'rdast' in descr:
        parsed_data['roof_deck_attch'] = 'st'  # standard
        descr = descr.replace('rdast', '')
    elif 'rdasu' in descr:
        parsed_data['roof_deck_attch'] = 'su'  # superior
        descr = descr.replace('rdasu', '')

    # Roof-Wall Connection
    if 'tnail' in descr:
        parsed_data['roof_wall_conn'] = 'tnail'
        descr = descr.replace('tnail', '')
    elif 'strap' in descr:
        parsed_data['roof_wall_conn'] = 'strap'
        descr = descr.replace('strap', '')

    # Garage
    if 'gdnod' in descr:
        parsed_data['garage'] = 'no'
        descr = descr.replace('gdnod', '')
    elif 'gdno2' in descr:
        parsed_data['garage'] = 'no'
        descr = descr.replace('gdno2', '')
    elif 'gdstd' in descr:
        parsed_data['garage'] = 'std'
        descr = descr.replace('gdstd', '')
    elif 'gdwkd' in descr:
        parsed_data['garage'] = 'wkd'
        descr = descr.replace('gdwkd', '')
    elif 'gdsup' in descr:
        parsed_data['garage'] = 'sup'
        descr = descr.replace('gdsup', '')

    # Shutters
    if 'shtys' in descr:
        parsed_data['shutters'] = True
        descr = descr.replace('shtys', '')
    elif 'shtno' in descr:
        parsed_data['shutters'] = False
        descr = descr.replace('shtno', '')

    # Roof Cover
    if 'rcbur' in descr:
        parsed_data['roof_cover'] = 'bur'
        descr = descr.replace('rcbur', '')
    elif 'rcspm' in descr:
        parsed_data['roof_cover'] = 'spm'
        descr = descr.replace('rcspm', '')

    # Roof Cover - Alternative Description
    if 'rcshl' in descr:
        parsed_data['roof_cover'] = 'cshl'  # cover, shingle
        descr = descr.replace('rcshl', '')
    elif 'rsmtl' in descr:
        parsed_data['roof_cover'] = 'smtl'  # sheet metal
        descr = descr.replace('rsmtl', '')

    # Roof Quality
    if 'rqgod' in descr:
        parsed_data['roof_quality'] = 'god'
        descr = descr.replace('rqgod', '')
    elif 'rqpor' in descr:
        parsed_data['roof_quality'] = 'por'
        descr = descr.replace('rqpor', '')

    # Masonry Reinforcing
    if 'rmfys' in descr:
        parsed_data['masonry_reinforcing'] = True
        descr = descr.replace('rmfys', '')
    elif 'rmfno' in descr:
        parsed_data['masonry_reinforcing'] = False
        descr = descr.replace('rmfno', '')

    # Roof Frame Type
    if 'rftrs' in descr:
        parsed_data['roof_frame_type'] = 'trs'  # wood truss
        descr = descr.replace('rftrs', '')
    elif 'rfows' in descr:
        parsed_data['roof_frame_type'] = 'ows'  # OWSJ
        descr = descr.replace('rfows', '')

    # Wind Debris Environment
    if 'widdA' in descr:
        parsed_data['wind_debris'] = 'A'  # res/comm.
        descr = descr.replace('widdA', '')
    elif 'widdB' in descr:
        parsed_data['wind_debris'] = 'B'  # varies by direction
        descr = descr.replace('widdB', '')
    elif 'widdC' in descr:
        parsed_data['wind_debris'] = 'C'  # residential
        descr = descr.replace('widdC', '')
    elif 'widdD' in descr:
        parsed_data['wind_debris'] = 'D'  # none
        descr = descr.replace('widdD', '')

    # Roof Deck Age
    if 'dqgod' in descr:
        parsed_data['roof_deck_age'] = 'god'  # new or average
        descr = descr.replace('dqgod', '')
    elif 'dqpor' in descr:
        parsed_data['roof_deck_age'] = 'por'  # old
        descr = descr.replace('dqpor', '')

    # Metal Roof Deck Attachment
    if 'rd100' in descr:
        parsed_data['metal_rda'] = 'std'  # standard
        descr = descr.replace('rd100', '')
    elif 'rd110' in descr:
        parsed_data['metal_rda'] = 'sup'  # superior
        descr = descr.replace('rd110', '')

    # Number of Units
    if 'nusgl' in descr:
        parsed_data['num_of_units'] = 'sgl'
        descr = descr.replace('nusgl', '')
    elif 'numlt' in descr:
        parsed_data['num_of_units'] = 'mlt'
        descr = descr.replace('numlt', '')

    # Joist Spacing
    if 'jspa4' in descr:
        parsed_data['joist_spacing'] = '4'
        descr = descr.replace('jspa4', '')
    elif 'jspa6' in descr:
        parsed_data['joist_spacing'] = '6'
        descr = descr.replace('jspa6', '')

    # Window Area
    if 'walow' in descr:
        parsed_data['window_area'] = 'low'
        descr = descr.replace('walow', '')
    elif 'wamed' in descr:
        parsed_data['window_area'] = 'med'
        descr = descr.replace('wamed', '')
    elif 'wahig' in descr:
        parsed_data['window_area'] = 'hig'
        descr = descr.replace('wahig', '')

    # ----- unknown attributes ---------

    if 'uprys' in descr:
        parsed_data['upgrade_??'] = True
        descr = descr.replace('uprys', '')
    elif 'uprno' in descr:
        parsed_data['upgrade_??'] = False
        descr = descr.replace('uprno', '')

    if 'wcdbl' in descr:
        parsed_data['wall_cover_??'] = 'dbl'
        descr = descr.replace('wcdbl', '')
    elif 'wcsgl' in descr:
        parsed_data['wall_cover_??'] = 'sgl'
        descr = descr.replace('wcsgl', '')

    if 'tspa2' in descr:
        parsed_data['tspa_??'] = '2'
        descr = descr.replace('tspa2', '')
    elif 'tspa4' in descr:
        parsed_data['tspa_??'] = '4'
        descr = descr.replace('tspa4', '')

    if 'mtdys' in descr:
        parsed_data['tie_downs'] = True
        descr = descr.replace('mtdys', '')
    elif 'mtdno' in descr:
        parsed_data['tie_downs'] = False
        descr = descr.replace('mtdno', '')

    return descr


def create_Hazus_HU_damage_and_loss_files():  # noqa: C901, D103, N802
    # Load RAW Hazus data

    raw_data_path = (
        'hurricane/building/portfolio/Hazus v4.2/data_sources/input_files/'
    )

    # read bldg data

    bldg_df_ST = pd.read_excel(  # noqa: N806
        raw_data_path + 'huListOfWindBldgTypes.xlsx', index_col=0
    )
    bldg_df_EF = pd.read_excel(  # noqa: N806
        raw_data_path + 'huListOfWindBldgTypesEF.xlsx', index_col=0
    )

    # make sure the column headers are in sync
    bldg_df_EF.columns = ['sbtName', *bldg_df_EF.columns[1:]]

    # offset the EF building IDs to ensure each archetype has a unique ID
    bldg_df_EF.index = max(bldg_df_ST.index) + bldg_df_EF.index
    bldg_df_EF.sort_index(inplace=True)

    bldg_df = pd.concat([bldg_df_ST, bldg_df_EF], axis=0)

    # read fragility data

    frag_df_ST = pd.read_excel(raw_data_path + 'huDamLossFun.xlsx')  # noqa: N806

    frag_df_EF = pd.read_excel(raw_data_path + 'huDamLossFunEF.xlsx')  # noqa: N806
    frag_df_EF['wbID'] += max(bldg_df_ST.index)

    frag_df = pd.concat([frag_df_ST, frag_df_EF], axis=0, ignore_index=True)

    frag_df.sort_values(['wbID', 'TERRAINID', 'DamLossDescID'], inplace=True)

    frag_df.reset_index(drop=True, inplace=True)

    # Fix errors and fill missing data in the raw fragility database

    # ## Incorrect Damage State labels
    #
    # **Problem**
    # Fragility curve data is stored with the wrong Damage State label
    # for some archetypes. This leads to a lower damage state having
    # higher corresponding capacity than a higher damage state.
    #
    # **Scope**
    # The problem only affects 40 archetypes. In all cases, the data
    # under Damage State 4 seems to belong to Damage State 1.
    #
    # **Fix**
    # We offset the Damage State 1-3 data by one DS and move what is
    # under Damage State 4 to DS1. This yields a plausible set of
    # fragility curves.
    #
    # Note: When identifying which archetypes are affected, we compare
    # the probabilities of exceeding each damage state at every
    # discrete wind speed in the database. We look for instances where
    # a lower damage state has lower probability of exceedance than a
    # higher damage state. When comparing probabilities of exceedance,
    # we recognize that the data in the Hazus database is noisy and
    # consider an absolute 2% tolerance to accommodate this noise and
    # avoid false positives.
    #

    # get labels of columns in frag_df with damage and loss data
    wind_speeds_str = [c for c in frag_df.columns if 'WS' in c]

    # also get a list of floats based on the above labels
    wind_speeds = np.array([float(ws[2:]) for ws in wind_speeds_str])

    # set the max wind speed of interest
    max_speed = 200
    max_speed_id = max(np.where(wind_speeds <= max_speed)[0]) + 1

    DS_data = [  # noqa: N806
        frag_df[frag_df['DamLossDescID'] == ds].loc[:, wind_speeds_str]
        for ds in range(1, 5)
    ]

    # the problem affects DS4 probabilities
    archetypes = (DS_data[2] - DS_data[3].values < -0.02).max(axis=1)
    # go through each affected archetype and fix the problem
    for frag_id in archetypes[archetypes == True].index:  # noqa
        # get the wbID and terrain_id
        wbID, terrain_id = frag_df.loc[frag_id, ['wbID', 'TERRAINID']]  # noqa: N806

        # load the fragility info for the archetype
        frag_df_arch = frag_df.loc[
            (frag_df['wbID'] == wbID) & (frag_df['TERRAINID'] == terrain_id)
        ]

        # check which DS is stored as DS4
        # we do this by looking at the median capacities at each DS
        # through simple interpolation
        median_capacities = [
            np.interp(
                0.5, frag_df_arch[wind_speeds_str].iloc[ds].values, wind_speeds
            )
            for ds in range(4)
        ]

        # then check where to store the values at DS4 to maintain
        # ascending exceedance probabilities
        target_DS = np.where(np.argsort(median_capacities) == 3)[0][0]  # noqa: N806

        # since this is always DS1 in the current database,
        # the script below works with that assumption and checks for exceptions
        if target_DS == 0:
            # first, extract the probabilities stored at DS4
            DS4_probs = frag_df_arch[wind_speeds_str].iloc[3].values  # noqa: N806

            # then offset the probabilities of DS1-3 by one level
            for ds in [3, 2, 1]:
                source_DS_index = frag_df_arch.index[ds - 1]  # noqa: N806
                target_DS_index = frag_df_arch.index[ds]  # noqa: N806

                frag_df.loc[target_DS_index, wind_speeds_str] = frag_df.loc[
                    source_DS_index, wind_speeds_str
                ].values

            # finally store the DS4 probs at the DS1 cells
            target_DS_index = frag_df_arch.index[0]  # noqa: N806

            frag_df.loc[target_DS_index, wind_speeds_str] = DS4_probs

    # ## Missing Damage State probabilities
    #
    # **Problem**
    # Some archetypes have only zeros in the cells that store Damage State
    # 4 exceedance probabilities at various wind speeds.
    #
    # **Scope**
    # This problem affects **346** building types in at least one but
    # typically all five terrain types. Altogether 1453 archetypes miss
    # their DS4 information in the raw data. As shown below, only DS4 is
    # affected, other damage states have some information for all
    # archetypes.
    #
    # **Fix**
    # We overwrite the zeros in the DS4 cells by copying the DS3
    # probabilities there. This leads to assuming zero probability of
    # exceeding DS4, which is still almost surely wrong. The **Hazus team
    # has been contacted** to provide the missing data or guidance on how
    # to improve this fix.

    # start by checking which archetypes have no damage data for at least
    # one Damage State

    # get the damage data for all archetypes
    DS_data = frag_df[frag_df['DamLossDescID'].isin([1, 2, 3, 4])].loc[  # noqa: N806
        :, wind_speeds_str
    ]

    # # check for invalid values
    # print(f'Global minimum value: {np.min(DS_data.values)}')
    # print(f'Global maximum value: {np.max(DS_data.values)}')

    # sum up the probabilities of exceeding each DS at various wind speeds
    DS_zero = DS_data.sum(axis=1)  # noqa: N806

    # and look for the lines where the sum is zero - i.e., all values are zero
    no_DS_info = frag_df.loc[DS_zero[DS_zero == 0].index]  # noqa: N806

    def overwrite_ds4_data():
        # now go through the building types in no_DS_info
        for wbID in no_DS_info['wbID'].unique():  # noqa: N806
            # and each terrain type that is affected
            for terrain_id in no_DS_info.loc[
                no_DS_info['wbID'] == wbID, 'TERRAINID'
            ].values:
                # get the fragility data for each archetype
                frag_df_arch = frag_df.loc[
                    (frag_df['wbID'] == wbID) & (frag_df['TERRAINID'] == terrain_id)
                ]

                # extract the DS3 information
                DS3_data = frag_df_arch.loc[  # noqa: N806
                    frag_df['DamLossDescID'] == 3, wind_speeds_str
                ].values

                # and overwrite the DS4 values in the original dataset
                DS4_index = frag_df_arch.loc[frag_df['DamLossDescID'] == 4].index  # noqa: N806
                frag_df.loc[DS4_index, wind_speeds_str] = DS3_data

    overwrite_ds4_data()

    # Fit fragility curves to discrete points

    # pre_calc

    flt_bldg_df = bldg_df.copy()

    # # this allows you to test it with a few archetypes before
    # # running the whole thing
    # flt_bldg_df = bldg_df.iloc[:100]

    # labels for all features, damage state data, and loss data
    column_names = [
        'bldg_type',
        'roof_shape',
        'roof_cover',
        'roof_quality',
        'sec_water_res',
        'roof_deck_attch',
        'roof_wall_conn',
        'garage',
        'shutters',
        'terr_rough',
        'upgrade_??',
        'wall_cover_??',
        'tspa_??',
        'masonry_reinforcing',
        'roof_frame_type',
        'wind_debris',
        'roof_deck_age',
        'metal_rda',
        'num_of_units',
        'joist_spacing',
        'window_area',
        'tie_downs',
        'DS1_dist',
        'DS1_mu',
        'DS1_sig',
        'DS1_fit',
        'DS1_meps',
        'DS2_dist',
        'DS2_mu',
        'DS2_sig',
        'DS2_fit',
        'DS2_meps',
        'DS3_dist',
        'DS3_mu',
        'DS3_sig',
        'DS3_fit',
        'DS3_meps',
        'DS4_dist',
        'DS4_mu',
        'DS4_sig',
        'DS4_fit',
        'DS4_meps',
        'L1',
        'L2',
        'L3',
        'L4',
        'L_fit',
        'L_meps',
        'DS1_original',
        'DS2_original',
        'DS3_original',
        'DS4_original',
        'L_original',
    ]

    # resulting dataframe
    new_df = pd.DataFrame(
        None, columns=column_names, index=np.arange(len(flt_bldg_df.index) * 5)
    )

    rows = []

    # calculation
    for index, row in tqdm(list(flt_bldg_df.iterrows())):
        # initialize the row for the archetype
        new_row = pd.Series(index=new_df.columns, dtype=np.float64)

        # store building type
        new_row['bldg_type'] = row['sbtName']

        # then parse the description and store the recognized parameter values
        descr = parse_description(row['charDescription'].strip(), new_row)

        # check if any part of the description remained unparsed
        if descr != '':
            print('WARNING', index, descr)

        # filter only those parts of the frag_df that correspond to
        # this archetype
        frag_df_arch = frag_df[frag_df['wbID'] == index]

        # cycle through the five terrain types in Hazus
        for terrain_id, roughness in enumerate([0.03, 0.15, 0.35, 0.7, 1.0]):
            # Hazus array indexing is 1-based
            terrain_id += 1

            new_row_terrain = new_row.copy()

            # store the roughness length
            new_row_terrain['terr_rough'] = roughness

            # filter only those parts of the frag_df_arch that correspond
            # to this terrain type
            frag_df_arch_terrain = frag_df_arch[
                frag_df_arch['TERRAINID'] == terrain_id
            ]

            mu_min = 0

            # for each damage state
            for DS in [1, 2, 3, 4]:  # noqa: N806
                # get the exceedence probabilities for this DS of this
                # archetype
                P_exc = np.asarray(  # noqa: N806
                    frag_df_arch_terrain.loc[
                        frag_df_arch_terrain['DamLossDescID'] == DS, wind_speeds_str
                    ].values[0]
                )
                multilinear_CDF_parameters = (  # noqa: N806
                    ','.join([str(x) for x in P_exc])
                    + '|'
                    + ','.join([str(x) for x in wind_speeds])
                )

                mu_0 = max(
                    [wind_speeds[np.argsort(abs(P_exc - 0.5))[0]], mu_min * 1.01]
                )
                sig_0 = 20
                beta_0 = 0.2

                median_id = max(np.where(wind_speeds <= mu_0)[0]) + 1
                min_speed_id = max(np.where(wind_speeds <= 100)[0]) + 1
                max_speed_id_mod = max(
                    [min([median_id, max_speed_id]), min_speed_id]
                )

                # define the two error measures to be minimized

                # assuming Normal distribution for building capacity
                def MSE_normal(params, mu_min, res_type='MSE'):  # noqa: N802
                    # unpack the parameters
                    mu, sig = params

                    # penalize invalid params
                    if (np.round(mu, decimals=1) <= mu_min) or (sig <= 0):
                        return 1e10

                    eps = (norm.cdf(wind_speeds, loc=mu, scale=sig) - P_exc)[  # noqa
                        :max_speed_id_mod  # noqa
                    ]

                    if res_type == 'MSE':
                        MSE = sum(eps**2.0)  # noqa: N806

                        return MSE

                    if res_type == 'max abs eps':
                        return max(abs(eps))

                    if res_type == 'eps':
                        return eps

                # assuming Lognormal distribution for building capacity
                def MSE_lognormal(params, mu_min, res_type='MSE'):  # noqa: N802
                    # unpack the parameters
                    mu, beta = params

                    # penalize invalid params
                    if (np.round(mu, decimals=1) <= mu_min) or (beta <= 0):
                        return 1e10

                    eps = (
                        norm.cdf(np.log(wind_speeds), loc=np.log(mu), scale=beta)
                        - P_exc  # noqa
                    )[
                        :max_speed_id_mod  # noqa
                    ]

                    if res_type == 'MSE':
                        MSE = sum(eps**2.0)  # noqa: N806

                        return MSE

                    if res_type == 'max abs eps':
                        return max(abs(eps))

                    if res_type == 'eps':
                        return eps

                # minimize MSE assuming Normal distribution
                res_normal = minimize(
                    MSE_normal,
                    [mu_0, sig_0],
                    args=(mu_min),
                    method='BFGS',
                    options={'maxiter': 50},
                )

                res_normal.x = np.array(
                    [
                        np.round(res_normal.x[0], decimals=1),
                        np.round(res_normal.x[1], decimals=1),
                    ]
                )

                # store MSE @ optimized location in fun
                res_normal.fun = np.sqrt(
                    MSE_normal(res_normal.x, mu_min) / max_speed_id_mod
                )
                # and hijack maxcv to store max eps within res object
                res_normal.maxcv = MSE_normal(
                    res_normal.x, mu_min, res_type='max abs eps'
                )

                # minimize MSE assuming Lognormal distribution
                res_lognormal = minimize(
                    MSE_lognormal,
                    [mu_0, beta_0],
                    args=(mu_min),
                    method='BFGS',
                    options={'maxiter': 50},
                )

                res_lognormal.x = np.array(
                    [
                        np.round(res_lognormal.x[0], decimals=1),
                        np.round(res_lognormal.x[1], decimals=2),
                    ]
                )

                # store the MSE @ optimized location in fun
                res_lognormal.fun = np.sqrt(
                    MSE_lognormal(res_lognormal.x, mu_min) / max_speed_id_mod
                )
                # and hijack maxcv to store max eps within res object
                res_lognormal.maxcv = MSE_lognormal(
                    res_lognormal.x, mu_min, res_type='max abs eps'
                )

                # show a warning if either model could not be fit
                if (res_normal.status != 0) and (res_lognormal.status != 0):
                    if (res_normal.fun < 1) or (res_lognormal.fun < 1):
                        pass

                    else:
                        print(
                            f'WARNING: Error in CDF fitting '
                            f'for {index}, {terrain_id}, {DS}'
                        )
                        # display(res_normal)
                        # display(res_lognormal)

                # keep the better fit:
                # first, check the mean absolute error
                if res_normal.fun < res_lognormal.fun:
                    # If the mean absolute errors in the two models
                    # are very close AND one model has substantially
                    # smaller maximum error than the other, then
                    # choose the model with the smaller maximum error
                    if (np.log(res_lognormal.fun / res_normal.fun) < 0.1) and (
                        np.log(res_normal.maxcv / res_lognormal.maxcv) > 0.1
                    ):
                        dist_type = 'lognormal'
                        res = res_lognormal

                    else:
                        dist_type = 'normal'
                        res = res_normal

                elif (np.log(res_normal.fun / res_lognormal.fun) < 0.1) and (
                    np.log(res_lognormal.maxcv / res_normal.maxcv) > 0.1
                ):
                    dist_type = 'normal'
                    res = res_normal

                else:
                    dist_type = 'lognormal'
                    res = res_lognormal

                # store the parameters
                new_row_terrain[f'DS{DS}_dist'] = dist_type
                new_row_terrain[f'DS{DS}_mu'] = res.x[0]
                new_row_terrain[f'DS{DS}_sig'] = res.x[1]
                new_row_terrain[f'DS{DS}_fit'] = res.fun
                new_row_terrain[f'DS{DS}_meps'] = res.maxcv
                new_row_terrain[f'DS{DS}_original'] = multilinear_CDF_parameters

                # consecutive damage states should have increasing capacities
                mu_min = res.x[0]

            # Now we have the damages, continue with Losses

            # Focus on "Building losses" first
            L_ref = np.asarray(  # noqa: N806
                frag_df_arch_terrain.loc[
                    frag_df_arch_terrain['DamLossDescID'] == 5, wind_speeds_str
                ].values[0]
            )

            multilinear_CDF_parameters = (  # noqa: N806
                ','.join([str(x) for x in L_ref])
                + '|'
                + ','.join([str(x) for x in wind_speeds])
            )

            # We'll need the probability of each Damage State across the
            # pre-defined wind speeds
            DS_probs = np.zeros((4, len(wind_speeds)))  # noqa: N806

            for DS_id, DS in enumerate([1, 2, 3, 4]):  # noqa: N806
                if new_row_terrain[f'DS{DS}_dist'] == 'normal':
                    DS_probs[DS_id] = norm.cdf(
                        wind_speeds,
                        loc=new_row_terrain[f'DS{DS}_mu'],
                        scale=new_row_terrain[f'DS{DS}_sig'],
                    )
                else:
                    DS_probs[DS_id] = norm.cdf(
                        np.log(wind_speeds),
                        loc=np.log(new_row_terrain[f'DS{DS}_mu']),
                        scale=new_row_terrain[f'DS{DS}_sig'],
                    )

            # The losses for DS4 are calculated based on outcomes at the
            # highest wind speeds
            L_max = frag_df_arch_terrain.loc[  # noqa: N806
                frag_df_arch_terrain['DamLossDescID'] == 5, 'WS250'
            ].values[0]
            DS4_max = DS_probs[3][-1]  # noqa: N806

            L4 = np.round(min(L_max / DS4_max, 1.0), decimals=3)  # noqa: N806

            # if L4 < 0.75:
            #    print(index, terrain_id, L_max, DS4_max, L4)

            for i in range(3):
                DS_probs[i] = DS_probs[i] - DS_probs[i + 1]

            # define the loss error measures to be minimized

            def SSE_loss(params, res_type='SSE'):  # noqa: N802
                loss_ratios = params.copy()

                # assume 1.0 for DS4
                loss_ratios = np.append(loss_ratios, L4)  # noqa

                L_est = np.sum(loss_ratios * DS_probs.T, axis=1)  # noqa

                # the error is the difference between reference and estimated losses
                eps = (L_est - L_ref)[:max_speed_id]  # noqa

                if res_type == 'SSE':
                    # calculate the sum of squared errors across wind speeds
                    SSE = sum(eps**2.0)  # noqa: N806

                elif res_type == 'max abs eps':
                    return max(abs(eps))

                return SSE

            cons = (
                {'type': 'ineq', 'fun': lambda x: x[1] - x[0] - 0.002},
                {'type': 'ineq', 'fun': lambda x: x[2] - x[1] - 0.002},
                {'type': 'ineq', 'fun': lambda x: L4 - x[2] - 0.002},  # noqa
            )

            res = minimize(
                SSE_loss,
                [0.02, 0.1, 0.44],
                bounds=((0.001, 1.0), (0.001, 1.0), (0.001, 1.0)),
                constraints=cons,
            )

            res.x = np.round(res.x, decimals=3)

            # store MSE @ optimized location in fun
            res.fun = np.sqrt(SSE_loss(res.x) / len(L_ref))
            # and hijack maxcv to store max eps within res object
            res.maxcv = SSE_loss(res.x, res_type='max abs eps')

            # if (res.fun > 0.1) or (res.maxcv > 0.2):
            #    print(index, terrain_id, max_speed_id_mod)
            #    print(res.x, res.fun, res.maxcv)

            # store the parameters
            new_row_terrain['L1'] = res.x[0]
            new_row_terrain['L2'] = res.x[1]
            new_row_terrain['L3'] = res.x[2]
            new_row_terrain['L4'] = L4
            new_row_terrain['L_original'] = multilinear_CDF_parameters
            new_row_terrain['L_fit'] = res.fun
            new_row_terrain['L_meps'] = res.maxcv

            # display(new_row.to_frame().T)

            rows.append(new_row_terrain.to_frame().T)

    main_df = pd.concat(rows, axis=0, ignore_index=True)

    # Prepare the Damage and Loss Model Data Files
    #
    # ## Prepare archetype IDs

    # This dict maps the building IDs used in HAZUS to the dotted
    # style we use in Pelicun

    bldg_type_map = {
        'WSF1': 'W.SF.1',
        'WSF2': 'W.SF.2',
        'WMUH1': 'W.MUH.1',
        'WMUH2': 'W.MUH.2',
        'WMUH3': 'W.MUH.3',
        'MSF1': 'M.SF.1',
        'MSF2': 'M.SF.2',
        'MMUH1': 'M.MUH.1',
        'MMUH2': 'M.MUH.2',
        'MMUH3': 'M.MUH.3',
        'MLRM1': 'M.LRM.1',
        'MLRM2': 'M.LRM.2',
        'MLRI': 'M.LRI',
        'MERBL': 'M.ERB.L',
        'MERBM': 'M.ERB.M',
        'MERBH': 'M.ERB.H',
        'MECBL': 'M.ECB.L',
        'MECBM': 'M.ECB.M',
        'MECBH': 'M.ECB.H',
        'CERBL': 'C.ERB.L',
        'CERBM': 'C.ERB.M',
        'CERBH': 'C.ERB.H',
        'CECBL': 'C.ECB.L',
        'CECBM': 'C.ECB.M',
        'CECBH': 'C.ECB.H',
        'SPMBS': 'S.PMB.S',
        'SPMBM': 'S.PMB.M',
        'SPMBL': 'S.PMB.L',
        'SERBL': 'S.ERB.L',
        'SERBM': 'S.ERB.M',
        'SERBH': 'S.ERB.H',
        'SECBL': 'S.ECB.L',
        'SECBM': 'S.ECB.M',
        'SECBH': 'S.ECB.H',
        'MHPHUD': 'MH.PHUD',
        'MH76HUD': 'MH.76HUD',
        'MH94HUDI': 'MH.94HUDI',
        'MH94HUDII': 'MH.94HUDII',
        'MH94HUDIII': 'MH.94HUDIII',
        'HUEFFS': 'HUEF.FS',
        'HUEFHS': 'HUEF.H.S',
        'HUEFHM': 'HUEF.H.M',
        'HUEFHL': 'HUEF.H.L',
        'HUEFSS': 'HUEF.S.S',
        'HUEFSM': 'HUEF.S.M',
        'HUEFSL': 'HUEF.S.L',
        'HUEFEO': 'HUEF.EO',
        'HUEFPS': 'HUEF.PS',
    }

    # beyond the building ID, we also want to capture the detailed
    # features of each archetype
    # with human-readable text (as opposed to what we had to parse in the
    # parse_description function earlier

    out_df = main_df.copy()
    out_df['ID'] = ''

    # some general formatting to make file name generation easier
    out_df['shutters'] = out_df['shutters'].astype(int)
    out_df['terr_rough'] = (out_df['terr_rough'] * 100.0).astype(int)

    for index, row in tqdm(list(out_df.iterrows())):
        # define the name of the building damage and loss configuration
        bldg_type = row['bldg_type']
        critical_cols = []

        if bldg_type[:3] == 'WSF':
            cols_of_interest = [
                'bldg_type',
                'roof_shape',
                'sec_water_res',
                'roof_deck_attch',
                'roof_wall_conn',
                'garage',
                'shutters',
                'terr_rough',
            ]
            critical_cols = ['roof_deck_attch', 'garage']

        elif bldg_type[:4] == 'WMUH':
            cols_of_interest = [
                'bldg_type',
                'roof_shape',
                'roof_cover',
                'roof_quality',
                'sec_water_res',
                'roof_deck_attch',
                'roof_wall_conn',
                'shutters',
                'terr_rough',
            ]

        elif bldg_type[:3] == 'MSF':
            cols_of_interest = [
                'bldg_type',
                'roof_shape',
                'roof_wall_conn',
                'roof_frame_type',
                'roof_deck_attch',
                'shutters',
                'sec_water_res',
                'garage',
                'masonry_reinforcing',
                'roof_cover',
                'terr_rough',
            ]

        elif bldg_type[:4] == 'MMUH':
            cols_of_interest = [
                'bldg_type',
                'roof_shape',
                'sec_water_res',
                'roof_cover',
                'roof_quality',
                'roof_deck_attch',
                'roof_wall_conn',
                'shutters',
                'masonry_reinforcing',
                'terr_rough',
            ]

        elif bldg_type[:5] == 'MLRM1':
            cols_of_interest = [
                'bldg_type',
                'roof_cover',
                'shutters',
                'masonry_reinforcing',
                'wind_debris',
                'roof_frame_type',
                'roof_deck_attch',
                'roof_wall_conn',
                'roof_deck_age',
                'metal_rda',
                'terr_rough',
            ]

        elif bldg_type[:5] == 'MLRM2':
            cols_of_interest = [
                'bldg_type',
                'roof_cover',
                'shutters',
                'masonry_reinforcing',
                'wind_debris',
                'roof_frame_type',
                'roof_deck_attch',
                'roof_wall_conn',
                'roof_deck_age',
                'metal_rda',
                'num_of_units',
                'joist_spacing',
                'terr_rough',
            ]

        elif bldg_type[:4] == 'MLRI':
            cols_of_interest = [
                'bldg_type',
                'shutters',
                'masonry_reinforcing',
                'roof_deck_age',
                'metal_rda',
                'terr_rough',
            ]

        elif bldg_type[:4] == 'MERB' or bldg_type[:4] == 'MECB':
            cols_of_interest = [
                'bldg_type',
                'roof_cover',
                'shutters',
                'wind_debris',
                'metal_rda',
                'window_area',
                'terr_rough',
            ]

        elif bldg_type[:4] == 'CERB' or bldg_type[:4] == 'CECB':
            cols_of_interest = [
                'bldg_type',
                'roof_cover',
                'shutters',
                'wind_debris',
                'window_area',
                'terr_rough',
            ]

        elif bldg_type[:4] == 'SERB' or bldg_type[:4] == 'SECB':
            cols_of_interest = [
                'bldg_type',
                'roof_cover',
                'shutters',
                'wind_debris',
                'metal_rda',
                'window_area',
                'terr_rough',
            ]

        elif bldg_type[:4] == 'SPMB':
            cols_of_interest = [
                'bldg_type',
                'shutters',
                'roof_deck_age',
                'metal_rda',
                'terr_rough',
            ]

        elif bldg_type[:2] == 'MH':
            cols_of_interest = ['bldg_type', 'shutters', 'tie_downs', 'terr_rough']

            critical_cols = [
                'tie_downs',
            ]

        elif bldg_type[:6] == 'HUEFFS':
            cols_of_interest = [
                'bldg_type',
                'roof_cover',
                'shutters',
                'wind_debris',
                'roof_deck_age',
                'metal_rda',
                'terr_rough',
            ]

        elif bldg_type[:6] == 'HUEFPS' or bldg_type[:6] == 'HUEFEO':
            cols_of_interest = [
                'bldg_type',
                'roof_cover',
                'shutters',
                'wind_debris',
                'metal_rda',
                'window_area',
                'terr_rough',
            ]

        elif bldg_type[:5] == 'HUEFH':
            cols_of_interest = [
                'bldg_type',
                'roof_cover',
                'wind_debris',
                'metal_rda',
                'shutters',
                'terr_rough',
            ]

        elif bldg_type[:5] == 'HUEFS':
            cols_of_interest = [
                'bldg_type',
                'roof_cover',
                'shutters',
                'wind_debris',
                'roof_deck_age',
                'metal_rda',
                'terr_rough',
            ]

        else:
            continue

        bldg_chars = row[cols_of_interest]

        # If a critical feature is undefined, we consider the archetype
        # unreliable and skip it
        skip_archetype = False
        for critical_col in critical_cols:
            if (critical_col in cols_of_interest) and (
                pd.isna(bldg_chars[critical_col])
            ):
                skip_archetype = True

        if skip_archetype:
            continue

        # Replace boolean strings with int equivalents
        bool_cols = ['sec_water_res', 'masonry_reinforcing', 'tie_downs']
        for col in bool_cols:
            if col in cols_of_interest:
                if pd.isna(bldg_chars[col]):
                    bldg_chars[col] = 'null'
                else:
                    bldg_chars[col] = int(bldg_chars[col])

        # Replace missing values with null
        for col in cols_of_interest:
            if pd.isna(bldg_chars[col]):
                bldg_chars[col] = 'null'

        # Add roof frame type info to MSF archetypes
        if bldg_type.startswith('MSF'):
            if bldg_chars['roof_cover'] in ['smtl', 'cshl']:
                bldg_chars['roof_frame_type'] = 'ows'
            else:
                bldg_chars['roof_frame_type'] = 'trs'

        bldg_chars['bldg_type'] = bldg_type_map[bldg_chars['bldg_type']]

        out_df.loc[index, 'ID'] = '.'.join(bldg_chars.astype(str))

    # the out_df we have in the end is the direct input to generate the
    # damage and loss data
    # -> so that we don't have to go through fitting everything every time
    # -> we need to re-generate the data for some reason
    out_df.head(10)

    # ## Export to CSV file

    # initialize the fragility table
    df_db_fit = pd.DataFrame(
        columns=[
            'ID',
            'Incomplete',
            'Demand-Type',
            'Demand-Unit',
            'Demand-Offset',
            'Demand-Directional',
            'LS1-Family',
            'LS1-Theta_0',
            'LS1-Theta_1',
            'LS2-Family',
            'LS2-Theta_0',
            'LS2-Theta_1',
            'LS3-Family',
            'LS3-Theta_0',
            'LS3-Theta_1',
            'LS4-Family',
            'LS4-Theta_0',
            'LS4-Theta_1',
        ],
        index=out_df.index,
        dtype=float,
    )

    df_db_original = pd.DataFrame(
        columns=[
            'ID',
            'Incomplete',
            'Demand-Type',
            'Demand-Unit',
            'Demand-Offset',
            'Demand-Directional',
            'LS1-Family',
            'LS1-Theta_0',
            'LS2-Family',
            'LS2-Theta_0',
            'LS3-Family',
            'LS3-Theta_0',
            'LS4-Family',
            'LS4-Theta_0',
        ],
        index=out_df.index,
        dtype=float,
    )

    for df_db in (df_db_original, df_db_fit):
        df_db['ID'] = out_df['ID']
        df_db['Incomplete'] = 0
        df_db['Demand-Type'] = 'Peak Gust Wind Speed'
        df_db['Demand-Unit'] = 'mph'
        df_db['Demand-Offset'] = 0
        df_db['Demand-Directional'] = 0

    for LS_i in range(1, 5):  # noqa: N806
        df_db_original[f'LS{LS_i}-Family'] = 'multilinear_CDF'
        df_db_original[f'LS{LS_i}-Theta_0'] = out_df[f'DS{LS_i}_original']

        df_db_fit[f'LS{LS_i}-Family'] = out_df[f'DS{LS_i}_dist']
        df_db_fit[f'LS{LS_i}-Theta_0'] = out_df[f'DS{LS_i}_mu']
        df_db_fit[f'LS{LS_i}-Theta_1'] = out_df[f'DS{LS_i}_sig']

    for df_db in (df_db_original, df_db_fit):
        df_db = df_db.loc[df_db['ID'] != '']
        df_db = df_db.set_index('ID').sort_index().convert_dtypes()

    df_db_fit.to_csv('hurricane/building/portfolio/Hazus v4.2/fragility_fitted.csv')
    df_db_original.to_csv(
        'hurricane/building/portfolio/Hazus v4.2/fragility_unmodified.csv'
    )

    # initialize the output loss table
    # define the columns
    out_cols = [
        'ID',
        'Incomplete',
        'Demand-Type',
        'Demand-Unit',
        'Demand-Offset',
        'Demand-Directional',
        'DV-Unit',
        'LossFunction-Theta_0',
    ]
    df_db_original = pd.DataFrame(columns=out_cols, index=out_df.index, dtype=float)
    df_db_original['ID'] = [f'{x}-Cost' for x in out_df['ID']]
    df_db_original['Incomplete'] = 0
    df_db_original['Demand-Type'] = 'Peak Gust Wind Speed'
    df_db_original['Demand-Unit'] = 'mph'
    df_db_original['Demand-Offset'] = 0
    df_db_original['Demand-Directional'] = 0
    df_db_original['DV-Unit'] = 'loss_ratio'
    df_db_original['LossFunction-Theta_0'] = out_df['L_original']
    df_db_original = df_db_original.loc[df_db_original['ID'] != '-Cost']
    df_db_original = df_db_original.set_index('ID').sort_index().convert_dtypes()

    out_cols = [
        'Incomplete',
        'Quantity-Unit',
        'DV-Unit',
    ]
    for DS_i in range(1, 5):  # noqa: N806
        out_cols += [f'DS{DS_i}-Theta_0']
    df_db_fit = pd.DataFrame(columns=out_cols, index=out_df.index, dtype=float)
    df_db_fit['ID'] = [f'{x}-Cost' for x in out_df['ID']]
    df_db_fit['Incomplete'] = 0
    df_db_fit['Quantity-Unit'] = '1 EA'
    df_db_fit['DV-Unit'] = 'loss_ratio'
    for LS_i in range(1, 5):  # noqa: N806
        df_db_fit[f'DS{LS_i}-Theta_0'] = out_df[f'L{LS_i}']
    df_db_fit = df_db_fit.loc[df_db_fit['ID'] != '-Cost']
    df_db_fit = df_db_fit.set_index('ID').sort_index().convert_dtypes()

    df_db_fit.to_csv(
        'hurricane/building/portfolio/Hazus v4.2/consequence_repair_fitted.csv'
    )
    df_db_original.to_csv(
        'hurricane/building/portfolio/Hazus v4.2/loss_repair_unmodified.csv'
    )


def create_Hazus_HU_metadata_files(  # noqa: C901, N802
    source_file: str = (
        'hurricane/building/portfolio/Hazus v4.2/fragility_fitted.csv'
    ),
    meta_file: str = (
        'hurricane/building/portfolio/Hazus v4.2/'
        'data_sources/input_files/metadata.json'
    ),
    target_meta_file_damage: str = (
        'hurricane/building/portfolio/Hazus v4.2/fragility_fitted.json'
    ),
    target_meta_file_loss: str = (
        'hurricane/building/portfolio/Hazus v4.2/consequence_repair_fitted.json'
    ),
    target_meta_file_damage_original: str = (
        'hurricane/building/portfolio/Hazus v4.2/fragility_unmodified.json'
    ),
    target_meta_file_loss_original: str = (
        'hurricane/building/portfolio/Hazus v4.2/loss_repair_unmodified.json'
    ),
) -> None:
    """
    Create a database metadata file for the HAZUS Hurricane fragilities.

    This method was developed to add a json file with metadata
    accompanying `damage_DB_SimCenter_Hazus_HU_bldg.csv`. That file
    contains fragility curves fitted to Hazus Hurricane data relaetd
    to the Hazus Hurricane Technical Manual v4.2.

    Parameters
    ----------
    source_file: string
        Path to the Hazus Hurricane fragility data.
    meta_file: string
        Path to a predefined fragility metadata file.
    target_meta_file_damage: string
        Path where the fitted fragility metadata should be saved. A
        json file is expected.
    target_meta_file_loss: string
        Path where the fitted loss damage consequence metadata should
        be saved. A json file is expected.
    target_meta_file_damage_original: string
        Path where the unmodified fragility metadata should be
        saved. A json file is expected.
    target_meta_file_loss_original: string
        Path where the unmodified loss function metadata should be
        saved. A json file is expected.

    """
    # Procedure Overview:
    # (1) We define several dictionaries mapping chunks of the
    # composite asset ID (the parts between periods) to human-readable
    # (`-h` for short) representations.
    # (2) We define -h asset type descriptions and map them to the
    # first-most relevant ID chunks (`primary chunks`)
    # (3) We map asset class codes with general asset classes
    # (4) We define the required dictionaries from (1) that decode the
    # ID chunks after the `primary chunks` for each general asset
    # class
    # (5) We decode:
    # ID -> asset class -> general asset class -> dictionaries
    # -> ID turns to -h text by combining the description of the asset class
    # from the `primary chunks` and the decoded description of the
    # following chunks using the dictionaries.

    #
    # (1) Dictionaries
    #

    roof_shape = {
        'flt': 'Flat roof.',
        'gab': 'Gable roof.',
        'hip': 'Hip roof.',
    }

    secondary_water_resistance = {
        '1': 'Secondary water resistance.',
        '0': 'No secondary water resistance.',
        'null': 'No information on secondary water resistance.',
    }

    roof_deck_attachment = {
        '6d': '6d roof deck nails.',
        '6s': '6s roof deck nails.',
        '8d': '8d roof deck nails.',
        '8s': '8s roof deck nails.',
        'st': 'Standard roof deck attachment.',
        'su': 'Superior roof deck attachment.',
        'null': 'Missing roof deck attachment information.',
    }

    roof_wall_connection = {
        'tnail': 'Roof-to-wall toe nails.',
        'strap': 'Roof-to-wall straps.',
        'null': 'Missing roof-to-wall connection information.',
    }

    garage_presence = {
        'no': 'No garage.',
        'wkd': 'Weak garage door.',
        'std': 'Standard garage door.',
        'sup': 'Strong garage door.',
        'null': 'No information on garage.',
    }

    shutters = {'1': 'Has Shutters.', '0': 'No shutters.'}

    roof_cover = {
        'bur': 'Built-up roof cover.',
        'spm': 'Single-ply membrane roof cover.',
        'smtl': 'Sheet metal roof cover.',
        'cshl': 'Shingle roof cover.',
        'null': 'No information on roof cover.',
    }

    roof_quality = {
        'god': 'Good roof quality.',
        'por': 'Poor roof quality.',
        'null': 'No information on roof quality.',
    }

    masonry_reinforcing = {
        '1': 'Has masonry reinforcing.',
        '0': 'No masonry reinforcing.',
        'null': 'Unknown information on masonry reinforcing.',
    }

    roof_frame_type = {
        'trs': 'Wood truss roof frame.',
        'ows': 'OWSJ roof frame.',
    }

    wind_debris_environment = {
        'A': 'Residentiao/commercial wind debris environment.',
        'B': 'Wind debris environment varies by direction.',
        'C': 'Residential wind debris environment.',
        'D': 'No wind debris environment.',
    }

    roof_deck_age = {
        'god': 'New or average roof age.',
        'por': 'Old roof age.',
        'null': 'Missing roof age information.',
    }

    roof_metal_deck_attachment_quality = {
        'std': 'Standard metal deck roof attachment.',
        'sup': 'Superior metal deck roof attachment.',
        'null': 'Missing roof attachment quality information.',
    }

    number_of_units = {
        'sgl': 'Single unit.',
        'mlt': 'Multi-unit.',
        'null': 'Unknown number of units.',
    }

    joist_spacing = {
        '4': '4 ft joist spacing.',
        '6': '6 ft foot joist spacing.',
        'null': 'Unknown joist spacing.',
    }

    window_area = {
        'low': 'Low window area.',
        'med': 'Medium window area.',
        'hig': 'High window area.',
    }

    tie_downs = {'1': 'Tie downs.', '0': 'No tie downs.'}

    terrain_surface_roughness = {
        '3': 'Terrain surface roughness: 0.03 m.',
        '15': 'Terrain surface roughness: 0.15 m.',
        '35': 'Terrain surface roughness: 0.35 m.',
        '70': 'Terrain surface roughness: 0.7 m.',
        '100': 'Terrain surface roughness: 1 m.',
    }

    #
    # (2) Asset type descriptions
    #

    # maps class type code to -h description
    class_types = {
        # ------------------------
        'W.SF.1': 'Wood, Single-family, One-story.',
        'W.SF.2': 'Wood, Single-family, Two or More Stories.',
        # ------------------------
        'W.MUH.1': 'Wood, Multi-Unit Housing, One-story.',
        'W.MUH.2': 'Wood, Multi-Unit Housing, Two Stories.',
        'W.MUH.3': 'Wood, Multi-Unit Housing, Three or More Stories.',
        # ------------------------
        'M.SF.1': 'Masonry, Single-family, One-story.',
        'M.SF.2': 'Masonry, Single-family, Two or More Stories.',
        # ------------------------
        'M.MUH.1': 'Masonry, Multi-Unit Housing, One-story.',
        'M.MUH.2': 'Masonry, Multi-Unit Housing, Two Stories.',
        'M.MUH.3': 'Masonry, Multi-Unit Housing, Three or More Stories.',
        # ------------------------
        'M.LRM.1': 'Masonry, Low-Rise Strip Mall, Up to 15 Feet.',
        'M.LRM.2': 'Masonry, Low-Rise Strip Mall, More than 15 Feet.',
        # ------------------------
        'M.LRI': 'Masonry, Low-Rise Industrial/Warehouse/Factory Buildings.',
        # ------------------------
        'M.ERB.L': (
            'Masonry, Engineered Residential Building, Low-Rise (1-2 Stories).'
        ),
        'M.ERB.M': (
            'Masonry, Engineered Residential Building, Mid-Rise (3-5 Stories).'
        ),
        'M.ERB.H': (
            'Masonry, Engineered Residential Building, High-Rise (6+ Stories).'
        ),
        # ------------------------
        'M.ECB.L': (
            'Masonry, Engineered Commercial Building, Low-Rise (1-2 Stories).'
        ),
        'M.ECB.M': (
            'Masonry, Engineered Commercial Building, Mid-Rise (3-5 Stories).'
        ),
        'M.ECB.H': (
            'Masonry, Engineered Commercial Building, High-Rise (6+ Stories).'
        ),
        # ------------------------
        'C.ERB.L': (
            'Concrete, Engineered Residential Building, Low-Rise (1-2 Stories).'
        ),
        'C.ERB.M': (
            'Concrete, Engineered Residential Building, Mid-Rise (3-5 Stories).'
        ),
        'C.ERB.H': (
            'Concrete, Engineered Residential Building, High-Rise (6+ Stories).'
        ),
        # ------------------------
        'C.ECB.L': (
            'Concrete, Engineered Commercial Building, Low-Rise (1-2 Stories).'
        ),
        'C.ECB.M': (
            'Concrete, Engineered Commercial Building, Mid-Rise (3-5 Stories).'
        ),
        'C.ECB.H': (
            'Concrete, Engineered Commercial Building, High-Rise (6+ Stories).'
        ),
        # ------------------------
        'S.PMB.S': 'Steel, Pre-Engineered Metal Building, Small.',
        'S.PMB.M': 'Steel, Pre-Engineered Metal Building, Medium.',
        'S.PMB.L': 'Steel, Pre-Engineered Metal Building, Large.',
        # ------------------------
        'S.ERB.L': 'Steel, Engineered Residential Building, Low-Rise (1-2 Stories).',
        'S.ERB.M': 'Steel, Engineered Residential Building, Mid-Rise (3-5 Stories).',
        'S.ERB.H': 'Steel, Engineered Residential Building, High-Rise (6+ Stories).',
        # ------------------------
        'S.ECB.L': 'Steel, Engineered Commercial Building, Low-Rise (1-2 Stories).',
        'S.ECB.M': 'Steel, Engineered Commercial Building, Mid-Rise (3-5 Stories).',
        'S.ECB.H': 'Steel, Engineered Commercial Building, High-Rise (6+ Stories).',
        # ------------------------
        'MH.PHUD': 'Manufactured Home, Pre-Housing and Urban Development (HUD).',
        'MH.76HUD': 'Manufactured Home, 1976 HUD.',
        'MH.94HUDI': 'Manufactured Home, 1994 HUD - Wind Zone I.',
        'MH.94HUDII': 'Manufactured Home, 1994 HUD - Wind Zone II.',
        'MH.94HUDIII': 'Manufactured Home, 1994 HUD - Wind Zone III.',
        # ------------------------
        'HUEF.H.S': 'Small Hospital, Hospital with fewer than 50 Beds.',
        'HUEF.H.M': 'Medium Hospital, Hospital with beds between 50 & 150.',
        'HUEF.H.L': 'Large Hospital, Hospital with more than 150 Beds.',
        # ------------------------
        'HUEF.S.S': 'Elementary School.',
        'HUEF.S.M': 'High school, two-story.',
        'HUEF.S.L': 'Large high school, three-story.',
        # ------------------------
        'HUEF.EO': 'Emergency Operation Centers.',
        'HUEF.FS': 'Fire Station.',
        'HUEF.PS': 'Police Station.',
        # ------------------------
    }

    def find_class_type(entry: str) -> str | None:
        """
        Find the class type code.

        Find the class type code from an entry string based on
        predefined patterns.

        Parameters
        ----------
        entry : str
            A string representing the entry, consisting of delimited
            segments that correspond to various attributes of an
            asset.

        Returns
        -------
        str or None
            The class type code if a matching pattern is found;
            otherwise, None if no pattern matches the input string.

        """
        entry_elements = entry.split('.')
        for nper in range(1, len(entry_elements)):
            first_parts = '.'.join(entry_elements[:nper])
            if first_parts in class_types:
                return first_parts
        return None

    #
    # (3) General asset class
    #

    # maps class code type to general class code
    general_classes = {
        # ------------------------
        'W.SF.1': 'WSF',
        'W.SF.2': 'WSF',
        # ------------------------
        'W.MUH.1': 'WMUH',
        'W.MUH.2': 'WMUH',
        'W.MUH.3': 'WMUH',
        # ------------------------
        'M.SF.1': 'MSF',
        'M.SF.2': 'MSF',
        # ------------------------
        'M.MUH.1': 'MMUH',
        'M.MUH.2': 'MMUH',
        'M.MUH.3': 'MMUH',
        # ------------------------
        'M.LRM.1': 'MLRM1',
        'M.LRM.2': 'MLRM2',
        # ------------------------
        'M.LRI': 'MLRI',
        # ------------------------
        'M.ERB.L': 'MERB',
        'M.ERB.M': 'MERB',
        'M.ERB.H': 'MERB',
        # ------------------------
        'M.ECB.L': 'MECB',
        'M.ECB.M': 'MECB',
        'M.ECB.H': 'MECB',
        # ------------------------
        'C.ERB.L': 'CERB',
        'C.ERB.M': 'CERB',
        'C.ERB.H': 'CERB',
        # ------------------------
        'C.ECB.L': 'CECB',
        'C.ECB.M': 'CECB',
        'C.ECB.H': 'CECB',
        # ------------------------
        'S.PMB.S': 'SPMB',
        'S.PMB.M': 'SPMB',
        'S.PMB.L': 'SPMB',
        # ------------------------
        'S.ERB.L': 'SERB',
        'S.ERB.M': 'SERB',
        'S.ERB.H': 'SERB',
        # ------------------------
        'S.ECB.L': 'SECB',
        'S.ECB.M': 'SECB',
        'S.ECB.H': 'SECB',
        # ------------------------
        'MH.PHUD': 'MH',
        'MH.76HUD': 'MH',
        'MH.94HUDI': 'MH',
        'MH.94HUDII': 'MH',
        'MH.94HUDIII': 'MH',
        # ------------------------
        'HUEF.H.S': 'HUEFH',
        'HUEF.H.M': 'HUEFH',
        'HUEF.H.L': 'HUEFH',
        # ------------------------
        'HUEF.S.S': 'HUEFS',
        'HUEF.S.M': 'HUEFS',
        'HUEF.S.L': 'HUEFS',
        # ------------------------
        'HUEF.EO': 'HUEFEO',
        'HUEF.FS': 'HUEFFS',
        'HUEF.PS': 'HUEFPS',
        # ------------------------
    }

    #
    # (4) Relevant dictionaries
    #

    # maps general class code to list of dicts where the -h attribute
    # descriptions will be pulled from
    dictionaries_of_interest = {
        'WSF': [
            roof_shape,
            secondary_water_resistance,
            roof_deck_attachment,
            roof_wall_connection,
            garage_presence,
            shutters,
            terrain_surface_roughness,
        ],
        'WMUH': [
            roof_shape,
            roof_cover,
            roof_quality,
            secondary_water_resistance,
            roof_deck_attachment,
            roof_wall_connection,
            shutters,
            terrain_surface_roughness,
        ],
        'MSF': [
            roof_shape,
            roof_wall_connection,
            roof_frame_type,
            roof_deck_attachment,
            shutters,
            secondary_water_resistance,
            garage_presence,
            masonry_reinforcing,
            roof_cover,
            terrain_surface_roughness,
        ],
        'MMUH': [
            roof_shape,
            secondary_water_resistance,
            roof_cover,
            roof_quality,
            roof_deck_attachment,
            roof_wall_connection,
            shutters,
            masonry_reinforcing,
            terrain_surface_roughness,
        ],
        'MLRM1': [
            roof_cover,
            shutters,
            masonry_reinforcing,
            wind_debris_environment,
            roof_frame_type,
            roof_deck_attachment,
            roof_wall_connection,
            roof_deck_age,
            roof_metal_deck_attachment_quality,
            terrain_surface_roughness,
        ],
        'MLRM2': [
            roof_cover,
            shutters,
            masonry_reinforcing,
            wind_debris_environment,
            roof_frame_type,
            roof_deck_attachment,
            roof_wall_connection,
            roof_deck_age,
            roof_metal_deck_attachment_quality,
            number_of_units,
            joist_spacing,
            terrain_surface_roughness,
        ],
        'MLRI': [
            shutters,
            masonry_reinforcing,
            roof_deck_age,
            roof_metal_deck_attachment_quality,
            terrain_surface_roughness,
        ],
        'MERB': [
            roof_cover,
            shutters,
            wind_debris_environment,
            roof_metal_deck_attachment_quality,
            window_area,
            terrain_surface_roughness,
        ],
        'MECB': [
            roof_cover,
            shutters,
            wind_debris_environment,
            roof_metal_deck_attachment_quality,
            window_area,
            terrain_surface_roughness,
        ],
        'CERB': [
            roof_cover,
            shutters,
            wind_debris_environment,
            window_area,
            terrain_surface_roughness,
        ],
        'CECB': [
            roof_cover,
            shutters,
            wind_debris_environment,
            window_area,
            terrain_surface_roughness,
        ],
        'SPMB': [
            shutters,
            roof_deck_age,
            roof_metal_deck_attachment_quality,
            terrain_surface_roughness,
        ],
        'SERB': [
            roof_cover,
            shutters,
            wind_debris_environment,
            roof_metal_deck_attachment_quality,
            window_area,
            terrain_surface_roughness,
        ],
        'SECB': [
            roof_cover,
            shutters,
            wind_debris_environment,
            roof_metal_deck_attachment_quality,
            window_area,
            terrain_surface_roughness,
        ],
        'MH': [shutters, tie_downs, terrain_surface_roughness],
        'HUEFH': [
            roof_cover,
            wind_debris_environment,
            roof_metal_deck_attachment_quality,
            shutters,
            terrain_surface_roughness,
        ],
        'HUEFS': [
            roof_cover,
            shutters,
            wind_debris_environment,
            roof_deck_age,
            roof_metal_deck_attachment_quality,
            terrain_surface_roughness,
        ],
        'HUEFEO': [
            roof_cover,
            shutters,
            wind_debris_environment,
            roof_metal_deck_attachment_quality,
            window_area,
            terrain_surface_roughness,
        ],
        'HUEFFS': [
            roof_cover,
            shutters,
            wind_debris_environment,
            roof_deck_age,
            roof_metal_deck_attachment_quality,
            terrain_surface_roughness,
        ],
        'HUEFPS': [
            roof_cover,
            shutters,
            wind_debris_environment,
            roof_metal_deck_attachment_quality,
            window_area,
            terrain_surface_roughness,
        ],
    }

    #
    # (5) Decode IDs and extend metadata with the individual records
    #

    # Create damage metadata

    fragility_data = pd.read_csv(source_file)

    with open(meta_file, encoding='utf-8') as f:
        meta_dict = json.load(f)

    # retrieve damage state descriptions and remove that part from
    # `hazus_hu_metadata`
    damage_state_classes = meta_dict.pop('DamageStateClasses')
    damage_state_descriptions = meta_dict.pop('DamageStateDescriptions')

    for fragility_id in fragility_data['ID'].to_list():
        # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
        # TODO
        # This is a temporary fix until we resolve the presence of NaN
        # values in the ID column of the fragility library file.
        if pd.isna(fragility_id):
            continue
        # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

        class_type = find_class_type(fragility_id)
        assert class_type is not None

        class_type_human_readable = class_types[class_type]

        general_class = general_classes[class_type]
        dictionaries = dictionaries_of_interest[general_class]
        remaining_chunks = fragility_id.replace(f'{class_type}.', '').split('.')
        assert len(remaining_chunks) == len(dictionaries)
        human_description = [class_type_human_readable]
        for chunk, dictionary in zip(remaining_chunks, dictionaries):
            human_description.append(dictionary[chunk])
        human_description_str = ' '.join(human_description)

        damage_state_class = damage_state_classes[class_type]
        damage_state_description = damage_state_descriptions[damage_state_class]

        limit_states = {}
        for damage_state, description in damage_state_description.items():
            limit_state = damage_state.replace('DS', 'LS')
            limit_states[limit_state] = {damage_state: description}

        record = {
            'Description': human_description_str,
            'SuggestedComponentBlockSize': '1 EA',
            'RoundUpToIntegerQuantity': 'True',
            'LimitStates': limit_states,
        }

        meta_dict[fragility_id] = record

    # save the metadata
    with open(target_meta_file_damage, 'w', encoding='utf-8') as f:
        json.dump(meta_dict, f, indent=2)

    # the unmodified damage state metadata are the same as those for
    # the fitted damage states.
    shutil.copy(target_meta_file_damage, target_meta_file_damage_original)

    # Create loss metadata & original loss function metadata

    with open(meta_file, encoding='utf-8') as f:
        meta_dict = json.load(f)

    # retrieve damage state descriptions and remove that part from
    # `hazus_hu_metadata`
    damage_state_classes = meta_dict.pop('DamageStateClasses')
    damage_state_descriptions = meta_dict.pop('DamageStateDescriptions')

    meta_dict_original = deepcopy(meta_dict)

    for fragility_id in fragility_data['ID'].to_list():
        # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
        # TODO
        # This is a temporary fix until we resolve the presence of NaN
        # values in the ID column of the fragility library file.
        if pd.isna(fragility_id):
            continue
        # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

        class_type = find_class_type(fragility_id)
        assert class_type is not None

        class_type_human_readable = class_types[class_type]

        general_class = general_classes[class_type]
        dictionaries = dictionaries_of_interest[general_class]
        remaining_chunks = fragility_id.replace(f'{class_type}.', '').split('.')
        assert len(remaining_chunks) == len(dictionaries)
        human_description = [class_type_human_readable]
        for chunk, dictionary in zip(remaining_chunks, dictionaries):
            human_description.append(dictionary[chunk])
        human_description_str = ' '.join(human_description)

        damage_state_class = damage_state_classes[class_type]
        damage_state_description = damage_state_descriptions[damage_state_class]

        damage_states = {}
        for damage_state, description in damage_state_description.items():
            damage_states[damage_state] = {'Description': description}

        record = {
            'Description': human_description_str,
            'SuggestedComponentBlockSize': '1 EA',
            'RoundUpToIntegerQuantity': 'True',
            'DamageStates': damage_states,
        }

        record_no_ds = {
            'Description': human_description_str,
            'SuggestedComponentBlockSize': '1 EA',
            'RoundUpToIntegerQuantity': 'True',
        }

        meta_dict[fragility_id] = record
        meta_dict_original[fragility_id] = record_no_ds

    # save the metadata
    with open(target_meta_file_loss, 'w', encoding='utf-8') as f:
        json.dump(meta_dict, f, indent=2)
    with open(target_meta_file_loss_original, 'w', encoding='utf-8') as f:
        json.dump(meta_dict_original, f, indent=2)


def main():
    """Generate Hazus Hurricane damage and loss database files."""
    create_Hazus_HU_damage_and_loss_files()
    create_Hazus_HU_metadata_files()


if __name__ == '__main__':
    main()
